{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1685184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with gzip.open(\"../dataset0.json.gz\", \"rt\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cbc4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, confusion_matrix)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c3b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_and_position(data):\n",
    "    id = list(data.keys())[0]\n",
    "    position = list(data[id].keys())[0]\n",
    "    return id, position\n",
    "\n",
    "def get_features(data):\n",
    "    id, position = get_id_and_position(data)\n",
    "    return list(data[id][position].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3cac049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.506295244505\n",
      "minimum number of reads is 20\n",
      "maximum number of reads is 991\n"
     ]
    }
   ],
   "source": [
    "feature_length = []\n",
    "for i in data:\n",
    "    feature_length.append(len(get_features(i)))\n",
    "\n",
    "print(np.mean(feature_length))\n",
    "print(f\"minimum number of reads is {np.min(feature_length)}\")\n",
    "print(f\"maximum number of reads is {np.max(feature_length)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbe93134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the labelled info file\n",
    "df_labels = pd.read_csv(\"../data.info.labelled\")\n",
    "\n",
    "label_dict = {\n",
    "    (row.transcript_id, row.transcript_position): row.label\n",
    "    for row in df_labels.itertuples(index=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c0b48aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (121838,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     X_data.append(features)\n\u001b[32m      9\u001b[39m     y_data.append(label)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m X_data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m y_data = np.array(y_data)\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (121838,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for entry in data:\n",
    "    id, position = get_id_and_position(entry)\n",
    "    features = get_features(entry)\n",
    "    label = label_dict.get((id, int(position)), None)\n",
    "    X_data.append(features)\n",
    "    y_data.append(label)\n",
    "    \n",
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c837170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# for apple silicon (M1 chips and newer)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "339e4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class M6ADataset(Dataset):\n",
    "    def __init__(self, X, y, max_length=200):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: List of arrays, each with shape (9, N) where N is variable\n",
    "            y: List of labels (0 or 1)\n",
    "            max_length: Maximum sequence length (pads shorter sequences)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]  # Shape: (N, 9) - N reads, 9 features\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        # Convert to numpy array if it's a list\n",
    "        x = np.array(x)\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        n_reads = x.shape[0]\n",
    "        if n_reads < self.max_length:\n",
    "            # Pad with zeros\n",
    "            padding = np.zeros((self.max_length - n_reads, 9))\n",
    "            x = np.vstack([x, padding])\n",
    "        else:\n",
    "            # Truncate to max_length\n",
    "            x = x[:self.max_length]\n",
    "        \n",
    "        # Store original length for masking (optional, for attention)\n",
    "        length = min(n_reads, self.max_length)\n",
    "        \n",
    "        return {\n",
    "            'X': torch.FloatTensor(x),\n",
    "            'y': torch.LongTensor([y]),\n",
    "            'length': length\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4b69099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size=9, hidden_size=64, num_layers=2, \n",
    "                dropout=0.3, output_size=2):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, input_size)\n",
    "            lengths: Tensor of actual sequence lengths for masking\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # LSTM forward pass\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Use last output\n",
    "        last_out = attn_out[:, -1, :]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.fc(last_out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a3bacc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        X = batch['X'].to(device)\n",
    "        y = batch['y'].squeeze().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X, batch['length'])\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            X = batch['X'].to(device)\n",
    "            y = batch['y'].squeeze().to(device)\n",
    "            \n",
    "            logits = model(X, batch['length'])\n",
    "            loss = criterion(logits, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, zero_division=0),\n",
    "        'auc': roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0\n",
    "    }\n",
    "    \n",
    "    return metrics, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8119065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_m6a_model(X_data, y_data, epochs=50, batch_size=16, \n",
    "                    max_length=200, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train the m6A detection model.\n",
    "    \n",
    "    Args:\n",
    "        X_data: List of numpy arrays with shape (9, N)\n",
    "        y_data: List of binary labels\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        max_length: Maximum sequence length\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        history: Training history\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize features\n",
    "    print(\"Normalizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = []\n",
    "    for x in X_data:\n",
    "        # Convert to numpy array if it's a list\n",
    "        x = np.array(x)  # Shape: (N, 9)\n",
    "        x_norm = scaler.fit_transform(x)  # Normalize each row independently\n",
    "        X_normalized.append(x_norm)\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_normalized, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    "    )\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = M6ADataset(X_train, y_train, max_length=max_length)\n",
    "    test_dataset = M6ADataset(X_test, y_test, max_length=max_length)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LSTMWithAttention(input_size=9, hidden_size=64, num_layers=2, \n",
    "                            dropout=0.3, output_size=2).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_f1': [],\n",
    "        'val_auc': []\n",
    "    }\n",
    "    \n",
    "    best_f1 = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\nTraining on {device}...\")\n",
    "    print(f\"Train set: {len(X_train)}, Test set: {len(X_test)}\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_metrics, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "        history['val_auc'].append(val_metrics['auc'])\n",
    "        \n",
    "        scheduler.step(val_metrics['f1'])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_metrics['loss']:.4f} | Acc: {val_metrics['accuracy']:.4f} | \"\n",
    "            f\"F1: {val_metrics['f1']:.4f} | AUC: {val_metrics['auc']:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_metrics['f1'] > best_f1:\n",
    "            best_f1 = val_metrics['f1']\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_m6a_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_m6a_model.pth'))\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    return model, history, test_loader, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "334d0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Metrics\n",
    "    axes[1].plot(history['val_accuracy'], label='Accuracy', marker='o')\n",
    "    axes[1].plot(history['val_f1'], label='F1 Score', marker='s')\n",
    "    axes[1].plot(history['val_auc'], label='AUC', marker='^')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].set_title('Validation Metrics')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3494e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing features...\n",
      "\n",
      "Training on mps...\n",
      "Train set: 97470, Test set: 24368\n",
      "\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.1686\n",
      "  Val Loss: 0.1501 | Acc: 0.9552 | F1: 0.0073 | AUC: 0.8201\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.1514\n",
      "  Val Loss: 0.1424 | Acc: 0.9563 | F1: 0.1461 | AUC: 0.8389\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.1431\n",
      "  Val Loss: 0.1437 | Acc: 0.9557 | F1: 0.0527 | AUC: 0.8435\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.1395\n",
      "  Val Loss: 0.1500 | Acc: 0.9566 | F1: 0.1950 | AUC: 0.8575\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.1352\n",
      "  Val Loss: 0.1391 | Acc: 0.9570 | F1: 0.3078 | AUC: 0.8666\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.1307\n",
      "  Val Loss: 0.1394 | Acc: 0.9572 | F1: 0.1885 | AUC: 0.8668\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.1262\n",
      "  Val Loss: 0.1375 | Acc: 0.9571 | F1: 0.1905 | AUC: 0.8528\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "model, history, test_loader, criterion = train_m6a_model(\n",
    "        X_data, y_data, epochs=50, batch_size=16, max_length=200, learning_rate=0.001\n",
    "    )\n",
    "\n",
    "plot_training_history(history)\n",
    "    \n",
    "    # Final evaluation\n",
    "final_metrics, preds, labels = evaluate(model, test_loader, criterion, device)\n",
    "print(\"\\nFinal Test Metrics:\")\n",
    "for key, val in final_metrics.items():\n",
    "    print(f\"  {key}: {val:.4f}\")\n",
    "    \n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(f\"\\nConfusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "903171b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_data[0]: <class 'list'>\n",
      "Shape of X_data[0]: 185\n",
      "Sample: [[0.00299, 2.06, 125.0, 0.0177, 10.4, 122.0, 0.0093, 10.9, 84.1], [0.00631, 2.53, 125.0, 0.00844, 4.67, 126.0, 0.0103, 6.3, 80.9]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type of X_data[0]: {type(X_data[0])}\")\n",
    "print(f\"Shape of X_data[0]: {len(X_data[0]) if isinstance(X_data[0], list) else X_data[0].shape}\")\n",
    "print(f\"Sample: {X_data[0][:2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
