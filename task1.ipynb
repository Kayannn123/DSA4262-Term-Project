{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0f35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, gzip\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "def parse_json_line(obj):\n",
    "    (tid, tdata), = obj.items()\n",
    "    (pos_key, contexts), = tdata.items()\n",
    "    pos = int(pos_key) if isinstance(pos_key, str) else pos_key\n",
    "    (ctx7, reads), = contexts.items()\n",
    "    arr = np.asarray(reads, dtype=float)\n",
    "    return tid, pos, ctx7, arr\n",
    "\n",
    "BASE_IDX = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\n",
    "def onehot28(seq7: str) -> np.ndarray:\n",
    "    out = np.zeros((7,4), dtype=np.int8)\n",
    "    s = (seq7 or \"\").upper()\n",
    "    for i in range(min(7, len(s))):\n",
    "        j = BASE_IDX.get(s[i], -1)\n",
    "        if j >= 0:\n",
    "            out[i, j] = 1\n",
    "    return out.ravel()\n",
    "\n",
    "def aggregate_9(arr: np.ndarray) -> np.ndarray:\n",
    "    if arr.size == 0:\n",
    "        return np.zeros(45, dtype=np.float32)\n",
    "    mean = arr.mean(axis=0)\n",
    "    std  = arr.std(axis=0, ddof=0)\n",
    "    mn   = arr.min(axis=0)\n",
    "    mx   = arr.max(axis=0)\n",
    "    med  = np.median(arr, axis=0)\n",
    "    return np.concatenate([mean, std, mn, mx, med]).astype(np.float32, copy=False)\n",
    "\n",
    "NUM_COLS = [\n",
    "    \"dwell_m1\",\"sd_m1\",\"mean_m1\",\n",
    "    \"dwell_0\",\"sd_0\",\"mean_0\",\n",
    "    \"dwell_p1\",\"sd_p1\",\"mean_p1\",\n",
    "]\n",
    "FEATURE_NAMES = (\n",
    "    [f\"mean_{c}\"   for c in NUM_COLS] +\n",
    "    [f\"std_{c}\"    for c in NUM_COLS] +\n",
    "    [f\"min_{c}\"    for c in NUM_COLS] +\n",
    "    [f\"max_{c}\"    for c in NUM_COLS] +\n",
    "    [f\"median_{c}\" for c in NUM_COLS] +\n",
    "    [f\"ctx_{i}\"    for i in range(28)]\n",
    ")\n",
    "\n",
    "\n",
    "def build_dataset_from_json_objects(json_objects, label_dict, transcript_to_gene):\n",
    "    \"\"\"\n",
    "    json_objects: iterable of parsed per-line dicts\n",
    "    label_dict: {(transcript_id, position): 0/1}\n",
    "    Returns X (N,73), y (N,), plus ids for later mapping.\n",
    "    \"\"\"\n",
    "    X_rows, y_rows, ids = [], [], []\n",
    "    for obj in json_objects:\n",
    "        tid, pos, ctx7, arr = parse_json_line(obj)\n",
    "        feats45 = aggregate_9(arr)\n",
    "        ctx28   = onehot28(ctx7)\n",
    "        X_rows.append(np.concatenate([feats45, ctx28]))\n",
    "        y_rows.append(label_dict.get((tid, int(pos)), None))\n",
    "        gene = transcript_to_gene.get((tid, None))\n",
    "        ids.append((gene, tid, int(pos)))\n",
    "    X = np.asarray(X_rows, dtype=np.float32)\n",
    "    y = np.asarray(y_rows)\n",
    "    return X, y, ids\n",
    "\n",
    "def train_and_save_model(X, y, ids, model_dir=\"models\"):\n",
    "    mask = ~pd.isna(y)\n",
    "    X_tr = X[mask]; y_tr = y[mask].astype(int)\n",
    "    \n",
    "    groups = np.array([\n",
    "        str(gene) if gene is not None else tid\n",
    "        for gene, tid, pos in ids\n",
    "    ])[mask]\n",
    "\n",
    "    gss = GroupShuffleSplit(test_size=0.3, random_state=42)\n",
    "    train_idx, temp_idx = next(gss.split(X_tr, y_tr, groups=groups))\n",
    "\n",
    "    gss2 = GroupShuffleSplit(test_size=0.5, random_state=42)\n",
    "    val_idx, test_idx = next(gss2.split(X_tr[temp_idx], y_tr[temp_idx], groups=groups[temp_idx]))\n",
    "\n",
    "    X_train, y_train = X_tr[train_idx], y_tr[train_idx]\n",
    "    X_val,   y_val   = X_tr[temp_idx][val_idx],  y_tr[temp_idx][val_idx]\n",
    "    X_test,  y_test  = X_tr[temp_idx][test_idx], y_tr[temp_idx][test_idx]\n",
    "\n",
    "    train_genes = set(groups[train_idx])\n",
    "    val_genes   = set(groups[temp_idx][val_idx])\n",
    "    test_genes  = set(groups[temp_idx][test_idx])\n",
    "    assert len(train_genes & val_genes) == 0, \"Gene overlap between train and val!\"\n",
    "    assert len(train_genes & test_genes) == 0, \"Gene overlap between train and test!\"\n",
    "    assert len(val_genes & test_genes) == 0, \"Gene overlap between val and test!\"\n",
    "    print(f\"Split complete: {len(train_genes)} train genes, {len(val_genes)} val genes, {len(test_genes)} test genes\")\n",
    "\n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric=\"aucpr\",\n",
    "        scale_pos_weight=20,\n",
    "        random_state=42,\n",
    "        n_jobs=os.cpu_count(), \n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    val_p = clf.predict_proba(X_val)[:,1]\n",
    "    tst_p = clf.predict_proba(X_test)[:,1]\n",
    "    print(\"VAL  AUPRC:\", average_precision_score(y_val, val_p), \"AUROC:\", roc_auc_score(y_val, val_p))\n",
    "    print(\"TEST AUPRC:\", average_precision_score(y_test, tst_p), \"AUROC:\", roc_auc_score(y_test, tst_p))\n",
    "\n",
    "    Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "    clf.save_model(Path(model_dir) / \"xgb_model.json\")\n",
    "    with open(Path(model_dir) / \"metadata.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"feature_names\": FEATURE_NAMES,\n",
    "            \"threshold\": 0.5\n",
    "        }, f, indent=2)\n",
    "    return clf\n",
    "\n",
    "def iter_json_lines(path: str):\n",
    "    \"\"\"Stream NDJSON from .json or .json.gz\"\"\"\n",
    "    p = Path(path)\n",
    "    if p.suffix == \".gz\":\n",
    "        with gzip.open(p, \"rt\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            for line in f:\n",
    "                s = line.strip()\n",
    "                if s:\n",
    "                    yield json.loads(s)\n",
    "    else:\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                s = line.strip()\n",
    "                if s:\n",
    "                    yield json.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84c3089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete: 3733 train genes, 800 val genes, 800 test genes\n",
      "VAL  AUPRC: 0.5373957333304621 AUROC: 0.9218604092017936\n",
      "TEST AUPRC: 0.45298139823266814 AUROC: 0.9269070576824326\n"
     ]
    }
   ],
   "source": [
    "df_labels = pd.read_csv(\"data_task1/data.info.labelled.csv\")\n",
    "label_dict = { (r.transcript_id, int(r.transcript_position)): int(r.label) for r in df_labels.itertuples(index=False)}\n",
    "transcript_to_gene = dict(zip(df_labels['transcript_id'], df_labels['gene_id']))\n",
    "\n",
    "train_json_iter = iter_json_lines(\"data_task1/dataset0.json\")\n",
    "X, y, ids = build_dataset_from_json_objects(train_json_iter, label_dict, transcript_to_gene)\n",
    "\n",
    "model = train_and_save_model(X, y, ids, model_dir=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff3cefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121838, 73)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee77b8c",
   "metadata": {},
   "source": [
    "#### Sanity Check\n",
    "- generate predictions for entire dataset\n",
    "- check prediction quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb5b719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dataset(model, data_path, label_path=None, output_csv=\"predictions/predictions_dataset0.csv\"):\n",
    "    json_iter = iter_json_lines(data_path)\n",
    "    X, y, ids = build_dataset_from_json_objects(json_iter, label_dict,transcript_to_gene)\n",
    "\n",
    "    preds = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"transcript_id\": [tid for gene, tid, pos in ids],\n",
    "        \"transcript_position\": [pos for gene, tid, pos in ids],\n",
    "        \"score\": preds\n",
    "    })\n",
    "    pred_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Predictions saved to {output_csv}\")\n",
    "\n",
    "    if label_path:\n",
    "        mask = ~pd.isna(y)\n",
    "        y_true = y[mask].astype(int)\n",
    "        y_pred = preds[mask]\n",
    "        auprc = average_precision_score(y_true, y_pred)\n",
    "        auroc = roc_auc_score(y_true, y_pred)\n",
    "        print(f\"[Sanity Check on labeled data]\")\n",
    "        print(f\"AUPRC: {auprc:.4f}, AUROC: {auroc:.4f}\")\n",
    "        return pred_df, auprc, auroc\n",
    "\n",
    "    return pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c79ff3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions/predictions_dataset0.csv\n",
      "[Sanity Check on labeled data]\n",
      "AUPRC: 0.8163, AUROC: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(          transcript_id  transcript_position     score\n",
       " 0       ENST00000000233                  244  0.005141\n",
       " 1       ENST00000000233                  261  0.051818\n",
       " 2       ENST00000000233                  316  0.006430\n",
       " 3       ENST00000000233                  332  0.080532\n",
       " 4       ENST00000000233                  368  0.008524\n",
       " ...                 ...                  ...       ...\n",
       " 121833  ENST00000641834                 1348  0.864682\n",
       " 121834  ENST00000641834                 1429  0.069511\n",
       " 121835  ENST00000641834                 1531  0.919931\n",
       " 121836  ENST00000641834                 1537  0.072776\n",
       " 121837  ENST00000641834                 1693  0.031175\n",
       " \n",
       " [121838 rows x 3 columns],\n",
       " np.float64(0.8162775099629662),\n",
       " np.float64(0.9763526120453562))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data_task1/dataset0.json\"\n",
    "label_path = \"data_task1/data.info.labelled.csv\"\n",
    "predict_dataset(model, data_path, label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1047d3f",
   "metadata": {},
   "source": [
    "#### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df189e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
